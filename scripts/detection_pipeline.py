import os, re, glob, xml.etree.ElementTree as ET
import numpy as np
import spacy
from pathlib import Path
from tqdm import tqdm
import faiss
from sentence_transformers import SentenceTransformer

# %% [markdown]
# å®šä¹‰æ–‡ä»¶è·¯å¾„å’Œè¶…å‚æ•°ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰

# %%

# è·¯å¾„è®¾ç½®ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
# æ–‡æœ¬å¯¹æ•°æ®ï¼ˆsusp, src, pairs.txtï¼‰æ‰€åœ¨ç›®å½•
DATA_DIR = '/kaggle/input/plagiarism-detection-data/pan25-generated-plagiarism-detection-validation/02_validation/02_validation/'

# æ ‡æ³¨ XML æ–‡ä»¶æ‰€åœ¨ç›®å½•
TRUTH_DIR = '/kaggle/input/plagiarism-detection-data/pan25-generated-plagiarism-detection-validation/02_validation/02_validation_truth'

# å­æ–‡ä»¶å¤¹è·¯å¾„
SUSP_DIR = os.path.join(DATA_DIR, 'susp')
SRC_DIR  = os.path.join(DATA_DIR, 'src')
PAIRS_FILE = os.path.join(DATA_DIR, 'pairs')

OUTPUT_DIR = '/kaggle/working/validation_pred_xml'  # è¾“å‡ºç»“æœä¿å­˜ç›®å½•
os.makedirs(OUTPUT_DIR, exist_ok=True)


EMB_DIR = "/kaggle/input/your-dataset-name/validation_doc_emb_cache"



WINDOW_SIZE = 6       # æ¯æ®µåŒ…å«å¥å­æ•°
STRIDE      = 2
SIM_THRESH  = 0.83    # ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼
MERGE_GAP   = 30      # åˆå¹¶æ£€æµ‹æ®µæ—¶ï¼Œå­—ç¬¦é—´éš” â‰¤ MERGE_GAP è§†ä¸ºåŒä¸€æ®µ
TOP_K = 12

# å‘é‡ç¼“å­˜ç›®å½•ï¼ˆä½ å¯ä»¥å­˜ä¸º dataset ä¸‹æ¬¡å¤ç”¨ï¼‰
VEC_CACHE_DIR = "/kaggle/working/vec_cache"
os.makedirs(VEC_CACHE_DIR, exist_ok=True)

# %%
# 4. å·¥å…·å‡½æ•°
# ========================================
def simple_sent_tokenize(text: str):
    text = re.sub(r'\s+', ' ', text.strip())
    return [s for s in re.split(r'(?<=[.!?])\s+', text) if s]

def load_and_split(path: str):
    with open(path, 'r', encoding='utf-8', errors='ignore') as f:
        txt = f.read()
    return txt, simple_sent_tokenize(txt)

def sliding_window(sentences, win=WINDOW_SIZE, stride=STRIDE):
    return [(i, " ".join(sentences[i:i+win]))
            for i in range(0, len(sentences) - win + 1, stride)]

def get_sentence_offsets(text, sentences):
    offsets, cur = [], 0
    for s in sentences:
        idx = text.find(s, cur)
        idx = idx if idx != -1 else cur
        offsets.append((idx, len(s)))
        cur = idx + len(s)
    return offsets

def cosine_sim_matrix(A: np.ndarray, B: np.ndarray):
    A = A / np.linalg.norm(A, axis=1, keepdims=True)
    B = B / np.linalg.norm(B, axis=1, keepdims=True)
    return A @ B.T

# def merge_spans(spans):
#     """åˆå¹¶ç›¸é‚»/é‡å /é—´éš”å¾ˆå°çš„å­—ç¬¦åŒºé—´åˆ—è¡¨"""
#     if not spans:
#         return []
#     spans.sort(key=lambda x: x[0])
#     merged = [spans[0]]
#     for start, end in spans[1:]:
#         last_start, last_end = merged[-1]
#         if start - last_end <= MERGE_GAP:
#             merged[-1] = (last_start, max(last_end, end))
#         else:
#             merged.append((start, end))
#     return merged


def merge_spans(spans, max_gap=50):
    """
    åˆå¹¶é‡å æˆ–æ¥è¿‘çš„ span æ®µï¼ˆç”¨äºåˆå¹¶æŠ„è¢­æ£€æµ‹ç»“æœï¼‰
    spans: List of (start, end)
    max_gap: ä¸¤ä¸ª span ä¹‹é—´å…è®¸çš„æœ€å¤§é—´éš™ï¼ˆå­—ç¬¦ï¼‰
    return: merged spans
    """
    if not spans:
        return []

    # æ’åº
    spans = sorted(spans, key=lambda x: x[0])
    merged = [spans[0]]

    for start, end in spans[1:]:
        prev_start, prev_end = merged[-1]

        # å¦‚æœå½“å‰æ®µå’Œä¸Šä¸€ä¸ªæ®µé‡å æˆ–é—´éš”ä¸è¶…è¿‡ max_gapï¼Œåˆ™åˆå¹¶
        if start <= prev_end + max_gap:
            merged[-1] = (prev_start, max(prev_end, end))
        else:
            merged.append((start, end))

    return merged

def save_xml(susp_name, src_name, feats, out_path):
    root = ET.Element('document', {'reference': susp_name})
    for f in feats:
        ET.SubElement(root, 'feature', {
            'name': 'detected-plagiarism',
            'this_offset': str(f['this_offset']),
            'this_length': str(f['this_length']),
            'source_reference': src_name,
            'source_offset': str(f['source_offset']),
            'source_length': str(f['source_length'])
        })
    ET.indent(ET.ElementTree(root), space="  ")
    ET.ElementTree(root).write(out_path, encoding='utf-8', xml_declaration=True)


def load_cached_doc_embedding(doc_id):
    path = os.path.join(EMB_DIR, f"{doc_id}.npy")
    return np.load(path)


# %% [markdown]
# **è¿™æ˜¯ç”¨è‡ªå·±çš„æ¨¡å‹å»è·‘****

# %%
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("intfloat/e5-base-v2", device='cuda')  # è‡ªåŠ¨ç”¨ GPU

def encode(texts):
    return model.encode(texts, batch_size=64, show_progress_bar=False, convert_to_numpy=True)

def get_or_encode_vectors(doc_path, sent_list, cache_dir, prefix):
    doc_id = Path(doc_path).stem
    out_path = os.path.join(cache_dir, f"{prefix}_{doc_id}.npy")

    if os.path.exists(out_path):
        return np.load(out_path)

    chunks = sliding_window(sent_list)
    texts = [chunk[1] for chunk in chunks]
    
    vecs = model.encode(texts, batch_size=64, show_progress_bar=False, convert_to_numpy=True).astype(np.float32)
    np.save(out_path, vecs)
    return vecs


# %%
import faiss

# ---------- 1) å…ˆå®šä¹‰ Faiss æœç´¢å‡½æ•°ï¼ˆå‡½æ•°çº§ã€é¡¶æ ¼ï¼‰ ----------
def faiss_search(query_vecs, db_vecs, top_k=5):
    dim = db_vecs.shape[1]
    index = faiss.IndexFlatIP(dim)          # å†…ç§¯ = ä½™å¼¦ï¼ˆéœ€å½’ä¸€åŒ–ï¼‰
    faiss.normalize_L2(query_vecs)
    faiss.normalize_L2(db_vecs)
    index.add(db_vecs)
    sims, indices = index.search(query_vecs, top_k)
    return sims, indices                    # shape: (n_query, top_k)

stats = {"matched": 0, "empty": 0}
# ---------- 2) å•å¯¹æ–‡æ¡£å¤„ç†å‡½æ•° ----------
def process_pair(susp_name, src_name):
     # æ–°å¢éƒ¨åˆ†ï¼šç¡®ä¿æºæ–‡ä»¶åç§°åŒ…å« .txt æ‰©å±•å
    if not src_name.endswith('.txt'):
        src_name = src_name + '.txt'
        
    susp_path = os.path.join(SUSP_DIR, susp_name)
    src_path  = os.path.join(SRC_DIR,  src_name)



    if not (os.path.exists(susp_path) and os.path.exists(src_path)):
        print("â— è·¯å¾„ä¸å­˜åœ¨:", susp_path, src_path)
        return

    # åˆ†å¥ä¸åç§»
    susp_text, susp_sents = load_and_split(susp_path)
    src_text,  src_sents  = load_and_split(src_path)
    susp_off = get_sentence_offsets(susp_text, susp_sents)
    src_off  = get_sentence_offsets(src_text,  src_sents)

    # æ»‘åŠ¨çª—å£ï¼ˆç”¨äºåç§»å®šä½ï¼‰
    susp_chunks = sliding_window(susp_sents)
    src_chunks  = sliding_window(src_sents)

    # åµŒå…¥ï¼ˆä¼˜å…ˆä»ç¼“å­˜è¯»å–ï¼‰
    susp_vecs = get_or_encode_vectors(susp_path, susp_sents, VEC_CACHE_DIR, prefix='susp')
    src_vecs  = get_or_encode_vectors(src_path,  src_sents,  VEC_CACHE_DIR, prefix='src')

    # Faiss æŸ¥è¯¢å€™é€‰åŒ¹é…çª—å£
    sims, idxs = faiss_search(susp_vecs, src_vecs, top_k=5)

    spans = []
    for i, (s_idx, _) in enumerate(susp_chunks):
        for score, j in zip(sims[i], idxs[i]):
            if score >= SIM_THRESH:
                r_idx, _ = src_chunks[j]
                s_start = susp_off[s_idx][0]
                s_end   = susp_off[s_idx + WINDOW_SIZE - 1][0] + susp_off[s_idx + WINDOW_SIZE - 1][1]
                r_start = src_off[r_idx][0]
                r_end   = src_off[r_idx + WINDOW_SIZE - 1][0] + src_off[r_idx + WINDOW_SIZE - 1][1]
                spans.append((s_start, s_end, r_start, r_end))

    # åˆå¹¶ + æ„å»º features
    merged = merge_spans([(s, e) for (s, e, _, _) in spans])
    det_features = []
    for m_start, m_end in merged:
        src_start = min(r_start for (s, e, r_start, r_end) in spans if s >= m_start and e <= m_end)
        src_end   = max(r_end   for (s, e, r_start, r_end) in spans if s >= m_start and e <= m_end)
        det_features.append({
            'this_offset': m_start,
            'this_length': m_end - m_start,
            'source_offset': src_start,
            'source_length': src_end - src_start
        })

    # ä¿å­˜ XML
    out_xml = os.path.join(OUTPUT_DIR, f"{Path(susp_name).stem}-{Path(src_name).stem}.xml")
    if det_features:
        save_xml(susp_name, src_name, det_features, out_xml)
        stats["matched"] += 1
    else:
        stats["empty"] += 1
        print(f"âš ï¸ æ— åŒ¹é…ç»“æœï¼Œæœªç”Ÿæˆ XML: {susp_name} - {src_name}")


def process_susp_filtered(susp_name):
    susp_path = os.path.join(SUSP_DIR, susp_name)
    top_src_names = get_top_k_sources(susp_path, k=5)

    for src_name in top_src_names:
        process_pair(susp_name, src_name)  # ä½ å·²æœ‰çš„ä¸»å‡½æ•°ä¸å˜


# %%
import xml.etree.ElementTree as ET

def save_detected_xml(susp_name, src_name, features, output_path):
    """
    å°†æ£€æµ‹åˆ°çš„æŠ„è¢­ç‰‡æ®µä¿¡æ¯ä¿å­˜ä¸ºæŒ‡å®šæ ¼å¼çš„ XMLã€‚
    features ä¸ºä¸€ç³»åˆ— dictï¼Œæ¯ä¸ªåŒ…å« this_offset, this_length, source_reference, source_offset, source_lengthã€‚
    """
    # åˆ›å»º XML æ ¹
    root = ET.Element('document', {'reference': susp_name})
    for feat in features:
        # æ·»åŠ  feature å…ƒç´ 
        ET.SubElement(root, 'feature', {
            'name': 'detected-plagiarism',
            'this_offset': feat['this_offset'],
            'this_length': feat['this_length'],
            'source_reference': feat['source_reference'],
            'source_offset': feat['source_offset'],
            'source_length': feat['source_length']
        })
    tree = ET.ElementTree(root)
    # ç¼©è¿›ï¼ˆPython 3.9+æ”¯æŒï¼Œé™ä½æ ¼å¼æ··ä¹±ï¼‰
    ET.indent(tree, space="  ", level=0)
    tree.write(output_path, encoding='utf-8', xml_declaration=True)
    print(f"å·²ä¿å­˜é¢„æµ‹ç»“æœï¼š{output_path}")

# ä¿å­˜å½“å‰ç¤ºä¾‹å¯¹çš„ç»“æœ
    output_file = os.path.join(OUTPUT_DIR, f"{susp_name.replace('.txt','')}-{src_name.replace('.txt','')}.xml")
    save_detected_xml(susp_name, src_name, detected_features, output_file)



# %% [markdown]
# 2ï¸âƒ£ å­é›†ç­›é€‰ + è¿‘ä¼¼å€™é€‰æºæ–‡æ¡£è¿‡æ»¤

# %%
def get_doc_embedding(path):
    text, _ = load_and_split(path)
    return model.encode([text], convert_to_numpy=True, normalize_embeddings=True)[0]


# %% [markdown]


# %%
# è·å–æ‰€æœ‰æºæ–‡æ¡£çš„æ•´ç¯‡åµŒå…¥
src_doc_paths = [os.path.join(SRC_DIR, f) for f in os.listdir(SRC_DIR) if f.endswith('.txt')]
src_doc_names = [os.path.basename(p) for p in src_doc_paths]
src_doc_vecs = []
for p in tqdm(src_doc_paths, desc="Embedding src docs", ncols=100):
    text, _ = load_and_split(p)
    vec = model.encode([text], convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
    src_doc_vecs.append(vec)

# %% [markdown]
# âœ… æ¨èä¿å­˜æ ¼å¼ï¼š.npz å‹ç¼©å‘é‡å­—å…¸
# âœ… ä¿å­˜åˆ° .npz æ–‡ä»¶ï¼ˆå½“å‰ Notebook æœ€åæ‰§è¡Œä¸€æ¬¡ï¼‰ï¼š

# %%
import numpy as np

# å»ºç«‹ ID â†’ å‘é‡çš„æ˜ å°„ï¼ˆdictï¼‰
src_doc_dict = {
    doc_name.replace(".txt", ""): vec
    for doc_name, vec in zip(src_doc_names, src_doc_vecs)
}

# ä¿å­˜ä¸ºå‹ç¼©æ–‡ä»¶
np.savez_compressed("validation_src_doc_vecs_e5.npz", **src_doc_dict)


# %% [markdown]
# âœ… ä¸‹æ¬¡ä½¿ç”¨æ—¶åªéœ€å‡ è¡Œä»£ç ï¼š

# %%
# æŒ‡å®šä½ ä¸Šä¼ çš„æ•°æ®é›†è·¯å¾„
VEC_PATH = "/kaggle/input/plagiarism-detection-data/validation_src_doc_vecs_e5.npz"
SUSP_VEC_DIR ="/kaggle/input/plagiarism-detection-data/susp_cache/kaggle/working/susp_emb_cache_e5"
# åŠ è½½æ‰€æœ‰åµŒå…¥
src_vec_npz = np.load(VEC_PATH)

# è·å–å•ç¯‡å‘é‡ï¼ˆä¾‹å¦‚ï¼šdocument123.txtï¼‰
# vec = src_vec_npz["document123"]
src_doc_names = list(src_vec_npz.keys())
src_doc_vecs = [src_vec_npz[k] for k in src_doc_names]


# %% [markdown]
# go on

# %%
# def get_top_k_sources(susp_path, k=5):
#     # susp_vec = get_doc_embedding(susp_path)  # shape: (384,)
#     susp_vec = np.load(os.path.join(SUSP_VEC_DIR, susp_name.replace(".txt", ".npy")))
#     sims = np.dot(src_doc_vecs, susp_vec)    # shape: (num_src,)
#     topk_idx = sims.argsort()[-k:][::-1].tolist()
#     # top_src_names = [src_doc_names[int(i) if not isinstance(i, list) else int(i[0])] for i in topk_idx]
#     # return top_src_names
#     # sims = np.dot(src_doc_vecs, susp_vec)
#     idxs = np.argsort(sims)[::-1][:top_k]
#     return [src_doc_names[i] for i in idxs]


# def get_top_k_sources(susp_name, k=10):
#     susp_vec_path = os.path.join(SUSP_VEC_DIR, susp_name.replace(".txt", ".npy"))
#     if not os.path.exists(susp_vec_path):
#         print(f"âŒ Susp å‘é‡ç¼ºå¤±: {susp_name}")
#         return []

#     susp_vec = np.load(susp_vec_path)  # shape: (768,)
    
#     sims = np.dot(src_doc_vecs, susp_vec)
#     idxs = np.argsort(sims)[::-1][:top_k]

#     return [src_doc_names[i] for i in idxs]

def get_top_k_sources(susp_name, k):
    susp_basename = os.path.basename(susp_name)
    susp_vec_path = os.path.join(SUSP_VEC_DIR, susp_basename.replace(".txt", ".npy"))

    if not os.path.exists(susp_vec_path):
        print(f"âŒ Susp å‘é‡ç¼ºå¤±: {susp_basename}")
        return []

    susp_vec = np.load(susp_vec_path)
    sims = np.dot(src_doc_vecs, susp_vec)
    idxs = np.argsort(sims)[::-1][:k]
    return [src_doc_names[i] for i in idxs]




# %%


# %%
from pathlib import Path
from sentence_transformers import SentenceTransformer
import numpy as np, os
from tqdm import tqdm

model = SentenceTransformer("intfloat/e5-base-v2")
model.to("cuda")


out_dir = "susp_emb_cache_e5"
os.makedirs(out_dir, exist_ok=True)

for fname in tqdm(os.listdir(SUSP_DIR)):
    if not fname.endswith(".txt"): continue
    path = os.path.join(SUSP_DIR, fname)
    with open(path, encoding="utf-8", errors="ignore") as f:
        text = f.read()

    emb = model.encode([text], convert_to_numpy=True,
                       normalize_embeddings=True)[0]
    np.save(os.path.join(out_dir, fname.replace(".txt", ".npy")), emb)


# %%
import os

# æ›¿æ¢ä¸ºå®é™…è·¯å¾„
vec_file = "/kaggle/input/plagiarism-detection-data/susp_cache/kaggle/working/susp_emb_cache_e5/suspicious-document010240.npy"
print(os.path.exists(vec_file))  # åº”è¯¥æ˜¯ True

# %%
from joblib import Parallel, delayed
from tqdm import tqdm
from tqdm_joblib import tqdm_joblib
import multiprocessing, time, os, warnings

import os
import warnings


# å±è”½æ‰€æœ‰ Python è­¦å‘Š
warnings.filterwarnings("ignore")


os.environ["TOKENIZERS_PARALLELISM"] = "false"  # æ¶ˆé™¤ tokenizer å¹¶è¡Œè­¦å‘Š
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"        # å…³é—­ TensorFlow åˆå§‹åŒ–é”™è¯¯è¾“å‡º

# ç¯å¢ƒé…ç½®
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
warnings.filterwarnings("ignore")
NUM_CORES = min(2, multiprocessing.cpu_count())

# åŠ è½½æ–‡æ¡£å¯¹
pairs = [tuple(line.strip().split()) for line in open(PAIRS_FILE, encoding='utf-8')]

with open(PAIRS_FILE, encoding='utf-8') as f:
    for line in f:
        susp_name, src_name = line.strip().split()
        # è‡ªåŠ¨è¡¥å…¨ .txtï¼ˆå¦‚æœç¼ºå¤±ï¼‰
        if not susp_name.endswith(".txt"):
            susp_name += ".txt"
        if not src_name.endswith(".txt"):
            src_name += ".txt"
        pairs.append((susp_name, src_name))

susp_files = sorted(set([s for s, _ in pairs]))

# åŒ…è£…å‡½æ•°
def safe_process_pair(susp_name, src_name):
    try:
        process_pair(susp_name, src_name)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {susp_name} - {src_name}ï¼Œ{e}")

NUM_CORES = 2  # Kaggle æœ€å¤š 2 æ ¸
start = time.time()

with tqdm_joblib(tqdm(total=len(pairs), desc="ğŸ“„ å¹¶è¡Œå¤„ç†æ–‡æ¡£å¯¹", unit="pair")):
    # å¹¶è¡Œï¼š
    Parallel(n_jobs=2)(
        delayed(process_susp_filtered)(susp_name)
        for susp_name in susp_files
    )


print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼æ€»è€—æ—¶ {time.time() - start:.2f} ç§’")
print(f"âœ… æ€»å¤„ç†æ•°ï¼š{len(pairs)}ï¼ŒæˆåŠŸå‘½ä¸­ï¼š{stats['matched']}ï¼Œæ— åŒ¹é…ï¼š{stats['empty']}")


# %%
import glob
print("å®é™…è¾“å‡ºæ–‡ä»¶æ•°é‡ï¼š", len(glob.glob(os.path.join(OUTPUT_DIR, "*.xml"))))


# %%
print("ç¤ºä¾‹è·¯å¾„ï¼š", os.path.join(SUSP_DIR, "suspicious-document020468.txt"))
print("å­˜åœ¨å—ï¼Ÿ", os.path.exists(os.path.join(SUSP_DIR, "suspicious-document020468.txt")))


# %% [markdown]
# æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°

# %%
# ç¤ºä¾‹ï¼šå‹ç¼© /kaggle/working/output_folder ä¸º output.zip
# !zip -r output.zip /kaggle/working/validation_pred_xml
