{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11971265,"sourceType":"datasetVersion","datasetId":7425488}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U sentence-transformers --quiet\n!pip install \"numpy<1.27\" \"scipy<1.11\" --force-reinstall --quiet\n!pip install --no-cache-dir sentence-transformers faiss-gpu --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:40:51.607465Z","iopub.execute_input":"2025-05-27T14:40:51.607694Z","iopub.status.idle":"2025-05-27T14:42:23.886265Z","shell.execute_reply.started":"2025-05-27T14:40:51.607677Z","shell.execute_reply":"2025-05-27T14:42:23.885514Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\nkaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\njax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\ncvxpy 1.6.4 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\njaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -U faiss-cpu sentence-transformers numpy==1.26.4 scipy==1.10.1 --quiet\n!pip install tqdm-joblib --quiet\n!pip install -U sentence-transformers numpy scikit-learn lxml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:42:23.887696Z","iopub.execute_input":"2025-05-27T14:42:23.887939Z","iopub.status.idle":"2025-05-27T14:42:45.198479Z","shell.execute_reply.started":"2025-05-27T14:42:23.887916Z","shell.execute_reply":"2025-05-27T14:42:45.197530Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nCollecting numpy\n  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nCollecting scikit-learn\n  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.3.1)\nCollecting lxml\n  Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.10.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\nDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lxml, scikit-learn\n  Attempting uninstall: lxml\n    Found existing installation: lxml 5.3.1\n    Uninstalling lxml-5.3.1:\n      Successfully uninstalled lxml-5.3.1\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed lxml-5.4.0 scikit-learn-1.6.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os, re, glob, xml.etree.ElementTree as ET\nimport numpy as np\nimport spacy\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport faiss\nfrom sentence_transformers import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:42:45.199594Z","iopub.execute_input":"2025-05-27T14:42:45.199882Z","iopub.status.idle":"2025-05-27T14:43:17.180508Z","shell.execute_reply.started":"2025-05-27T14:42:45.199832Z","shell.execute_reply":"2025-05-27T14:43:17.179881Z"}},"outputs":[{"name":"stderr","text":"2025-05-27 14:43:02.275245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748356982.490900      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748356982.560085      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"定义文件路径和超参数（根据实际情况修改）","metadata":{}},{"cell_type":"code","source":"\n# 路径设置（根据实际情况修改）\n# 文本对数据（susp, src, pairs.txt）所在目录\nDATA_DIR = '/kaggle/input/plagiarism-detection-data/pan25-generated-plagiarism-detection-validation/02_validation/02_validation/'\n\n# 标注 XML 文件所在目录\nTRUTH_DIR = '/kaggle/input/plagiarism-detection-data/pan25-generated-plagiarism-detection-validation/02_validation/02_validation_truth'\n\n# 子文件夹路径\nSUSP_DIR = os.path.join(DATA_DIR, 'susp')\nSRC_DIR  = os.path.join(DATA_DIR, 'src')\nPAIRS_FILE = os.path.join(DATA_DIR, 'pairs')\n\nOUTPUT_DIR = '/kaggle/working/validation_pred_xml'  # 输出结果保存目录\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\nEMB_DIR = \"/kaggle/input/your-dataset-name/validation_doc_emb_cache\"\n\n\n\nWINDOW_SIZE = 6       # 每段包含句子数\nSTRIDE      = 2\nSIM_THRESH  = 0.83    # 余弦相似度阈值\nMERGE_GAP   = 30      # 合并检测段时，字符间隔 ≤ MERGE_GAP 视为同一段\nTOP_K = 12\n\n# 向量缓存目录（你可以存为 dataset 下次复用）\nVEC_CACHE_DIR = \"/kaggle/working/vec_cache\"\nos.makedirs(VEC_CACHE_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:43:17.182191Z","iopub.execute_input":"2025-05-27T14:43:17.182667Z","iopub.status.idle":"2025-05-27T14:43:17.188292Z","shell.execute_reply.started":"2025-05-27T14:43:17.182649Z","shell.execute_reply":"2025-05-27T14:43:17.187582Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 4. 工具函数\n# ========================================\ndef simple_sent_tokenize(text: str):\n    text = re.sub(r'\\s+', ' ', text.strip())\n    return [s for s in re.split(r'(?<=[.!?])\\s+', text) if s]\n\ndef load_and_split(path: str):\n    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n        txt = f.read()\n    return txt, simple_sent_tokenize(txt)\n\ndef sliding_window(sentences, win=WINDOW_SIZE, stride=STRIDE):\n    return [(i, \" \".join(sentences[i:i+win]))\n            for i in range(0, len(sentences) - win + 1, stride)]\n\ndef get_sentence_offsets(text, sentences):\n    offsets, cur = [], 0\n    for s in sentences:\n        idx = text.find(s, cur)\n        idx = idx if idx != -1 else cur\n        offsets.append((idx, len(s)))\n        cur = idx + len(s)\n    return offsets\n\ndef cosine_sim_matrix(A: np.ndarray, B: np.ndarray):\n    A = A / np.linalg.norm(A, axis=1, keepdims=True)\n    B = B / np.linalg.norm(B, axis=1, keepdims=True)\n    return A @ B.T\n\n# def merge_spans(spans):\n#     \"\"\"合并相邻/重叠/间隔很小的字符区间列表\"\"\"\n#     if not spans:\n#         return []\n#     spans.sort(key=lambda x: x[0])\n#     merged = [spans[0]]\n#     for start, end in spans[1:]:\n#         last_start, last_end = merged[-1]\n#         if start - last_end <= MERGE_GAP:\n#             merged[-1] = (last_start, max(last_end, end))\n#         else:\n#             merged.append((start, end))\n#     return merged\n\n\ndef merge_spans(spans, max_gap=50):\n    \"\"\"\n    合并重叠或接近的 span 段（用于合并抄袭检测结果）\n    spans: List of (start, end)\n    max_gap: 两个 span 之间允许的最大间隙（字符）\n    return: merged spans\n    \"\"\"\n    if not spans:\n        return []\n\n    # 排序\n    spans = sorted(spans, key=lambda x: x[0])\n    merged = [spans[0]]\n\n    for start, end in spans[1:]:\n        prev_start, prev_end = merged[-1]\n\n        # 如果当前段和上一个段重叠或间隔不超过 max_gap，则合并\n        if start <= prev_end + max_gap:\n            merged[-1] = (prev_start, max(prev_end, end))\n        else:\n            merged.append((start, end))\n\n    return merged\n\ndef save_xml(susp_name, src_name, feats, out_path):\n    root = ET.Element('document', {'reference': susp_name})\n    for f in feats:\n        ET.SubElement(root, 'feature', {\n            'name': 'detected-plagiarism',\n            'this_offset': str(f['this_offset']),\n            'this_length': str(f['this_length']),\n            'source_reference': src_name,\n            'source_offset': str(f['source_offset']),\n            'source_length': str(f['source_length'])\n        })\n    ET.indent(ET.ElementTree(root), space=\"  \")\n    ET.ElementTree(root).write(out_path, encoding='utf-8', xml_declaration=True)\n\n\ndef load_cached_doc_embedding(doc_id):\n    path = os.path.join(EMB_DIR, f\"{doc_id}.npy\")\n    return np.load(path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:43:17.189152Z","iopub.execute_input":"2025-05-27T14:43:17.189420Z","iopub.status.idle":"2025-05-27T14:43:17.440090Z","shell.execute_reply.started":"2025-05-27T14:43:17.189396Z","shell.execute_reply":"2025-05-27T14:43:17.439247Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"**这是用自己的模型去跑****","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"intfloat/e5-base-v2\", device='cuda')  # 自动用 GPU\n\ndef encode(texts):\n    return model.encode(texts, batch_size=64, show_progress_bar=False, convert_to_numpy=True)\n\ndef get_or_encode_vectors(doc_path, sent_list, cache_dir, prefix):\n    doc_id = Path(doc_path).stem\n    out_path = os.path.join(cache_dir, f\"{prefix}_{doc_id}.npy\")\n\n    if os.path.exists(out_path):\n        return np.load(out_path)\n\n    chunks = sliding_window(sent_list)\n    texts = [chunk[1] for chunk in chunks]\n    \n    vecs = model.encode(texts, batch_size=64, show_progress_bar=False, convert_to_numpy=True).astype(np.float32)\n    np.save(out_path, vecs)\n    return vecs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:43:17.440862Z","iopub.execute_input":"2025-05-27T14:43:17.441199Z","iopub.status.idle":"2025-05-27T14:43:27.630797Z","shell.execute_reply.started":"2025-05-27T14:43:17.441171Z","shell.execute_reply":"2025-05-27T14:43:27.630018Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0e057640a2749bfb97f879d74eaa3a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/67.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"475f5a1db6f74c20b9a588e7aed76854"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1991dcfc7461475da3bbef33eb1b6063"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02e844f17a384251b87ebf20aa9dd2d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42d7d7a39caa471eabdd9f36760b0c2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bd41c83b62f4cd887b3c79031901c45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08083fa4e7c842a590c8f47fb9f14a5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfca0f21a30a46c186d243b610b31786"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ae0b4321b184123bbe762a0af9b66e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8676c82804b413e85268036e7eb64fe"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import faiss\n\n# ---------- 1) 先定义 Faiss 搜索函数（函数级、顶格） ----------\ndef faiss_search(query_vecs, db_vecs, top_k=5):\n    dim = db_vecs.shape[1]\n    index = faiss.IndexFlatIP(dim)          # 内积 = 余弦（需归一化）\n    faiss.normalize_L2(query_vecs)\n    faiss.normalize_L2(db_vecs)\n    index.add(db_vecs)\n    sims, indices = index.search(query_vecs, top_k)\n    return sims, indices                    # shape: (n_query, top_k)\n\nstats = {\"matched\": 0, \"empty\": 0}\n# ---------- 2) 单对文档处理函数 ----------\ndef process_pair(susp_name, src_name):\n     # 新增部分：确保源文件名称包含 .txt 扩展名\n    if not src_name.endswith('.txt'):\n        src_name = src_name + '.txt'\n        \n    susp_path = os.path.join(SUSP_DIR, susp_name)\n    src_path  = os.path.join(SRC_DIR,  src_name)\n\n\n\n    if not (os.path.exists(susp_path) and os.path.exists(src_path)):\n        print(\"❗ 路径不存在:\", susp_path, src_path)\n        return\n\n    # 分句与偏移\n    susp_text, susp_sents = load_and_split(susp_path)\n    src_text,  src_sents  = load_and_split(src_path)\n    susp_off = get_sentence_offsets(susp_text, susp_sents)\n    src_off  = get_sentence_offsets(src_text,  src_sents)\n\n    # 滑动窗口（用于偏移定位）\n    susp_chunks = sliding_window(susp_sents)\n    src_chunks  = sliding_window(src_sents)\n\n    # 嵌入（优先从缓存读取）\n    susp_vecs = get_or_encode_vectors(susp_path, susp_sents, VEC_CACHE_DIR, prefix='susp')\n    src_vecs  = get_or_encode_vectors(src_path,  src_sents,  VEC_CACHE_DIR, prefix='src')\n\n    # Faiss 查询候选匹配窗口\n    sims, idxs = faiss_search(susp_vecs, src_vecs, top_k=5)\n\n    spans = []\n    for i, (s_idx, _) in enumerate(susp_chunks):\n        for score, j in zip(sims[i], idxs[i]):\n            if score >= SIM_THRESH:\n                r_idx, _ = src_chunks[j]\n                s_start = susp_off[s_idx][0]\n                s_end   = susp_off[s_idx + WINDOW_SIZE - 1][0] + susp_off[s_idx + WINDOW_SIZE - 1][1]\n                r_start = src_off[r_idx][0]\n                r_end   = src_off[r_idx + WINDOW_SIZE - 1][0] + src_off[r_idx + WINDOW_SIZE - 1][1]\n                spans.append((s_start, s_end, r_start, r_end))\n\n    # 合并 + 构建 features\n    merged = merge_spans([(s, e) for (s, e, _, _) in spans])\n    det_features = []\n    for m_start, m_end in merged:\n        src_start = min(r_start for (s, e, r_start, r_end) in spans if s >= m_start and e <= m_end)\n        src_end   = max(r_end   for (s, e, r_start, r_end) in spans if s >= m_start and e <= m_end)\n        det_features.append({\n            'this_offset': m_start,\n            'this_length': m_end - m_start,\n            'source_offset': src_start,\n            'source_length': src_end - src_start\n        })\n\n    # 保存 XML\n    out_xml = os.path.join(OUTPUT_DIR, f\"{Path(susp_name).stem}-{Path(src_name).stem}.xml\")\n    if det_features:\n        save_xml(susp_name, src_name, det_features, out_xml)\n        stats[\"matched\"] += 1\n    else:\n        stats[\"empty\"] += 1\n        print(f\"⚠️ 无匹配结果，未生成 XML: {susp_name} - {src_name}\")\n\n\ndef process_susp_filtered(susp_name):\n    susp_path = os.path.join(SUSP_DIR, susp_name)\n    top_src_names = get_top_k_sources(susp_path, k=5)\n\n    for src_name in top_src_names:\n        process_pair(susp_name, src_name)  # 你已有的主函数不变\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:43:27.631764Z","iopub.execute_input":"2025-05-27T14:43:27.631998Z","iopub.status.idle":"2025-05-27T14:43:27.645554Z","shell.execute_reply.started":"2025-05-27T14:43:27.631979Z","shell.execute_reply":"2025-05-27T14:43:27.644782Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\n\ndef save_detected_xml(susp_name, src_name, features, output_path):\n    \"\"\"\n    将检测到的抄袭片段信息保存为指定格式的 XML。\n    features 为一系列 dict，每个包含 this_offset, this_length, source_reference, source_offset, source_length。\n    \"\"\"\n    # 创建 XML 根\n    root = ET.Element('document', {'reference': susp_name})\n    for feat in features:\n        # 添加 feature 元素\n        ET.SubElement(root, 'feature', {\n            'name': 'detected-plagiarism',\n            'this_offset': feat['this_offset'],\n            'this_length': feat['this_length'],\n            'source_reference': feat['source_reference'],\n            'source_offset': feat['source_offset'],\n            'source_length': feat['source_length']\n        })\n    tree = ET.ElementTree(root)\n    # 缩进（Python 3.9+支持，降低格式混乱）\n    ET.indent(tree, space=\"  \", level=0)\n    tree.write(output_path, encoding='utf-8', xml_declaration=True)\n    print(f\"已保存预测结果：{output_path}\")\n\n# 保存当前示例对的结果\n    output_file = os.path.join(OUTPUT_DIR, f\"{susp_name.replace('.txt','')}-{src_name.replace('.txt','')}.xml\")\n    save_detected_xml(susp_name, src_name, detected_features, output_file)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:43:27.646311Z","iopub.execute_input":"2025-05-27T14:43:27.646601Z","iopub.status.idle":"2025-05-27T14:43:27.669939Z","shell.execute_reply.started":"2025-05-27T14:43:27.646584Z","shell.execute_reply":"2025-05-27T14:43:27.669397Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"2️⃣ 子集筛选 + 近似候选源文档过滤","metadata":{}},{"cell_type":"code","source":"def get_doc_embedding(path):\n    text, _ = load_and_split(path)\n    return model.encode([text], convert_to_numpy=True, normalize_embeddings=True)[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:43:27.670604Z","iopub.execute_input":"2025-05-27T14:43:27.670813Z","iopub.status.idle":"2025-05-27T14:43:27.685037Z","shell.execute_reply.started":"2025-05-27T14:43:27.670790Z","shell.execute_reply":"2025-05-27T14:43:27.684398Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# 获取所有源文档的整篇嵌入\nsrc_doc_paths = [os.path.join(SRC_DIR, f) for f in os.listdir(SRC_DIR) if f.endswith('.txt')]\nsrc_doc_names = [os.path.basename(p) for p in src_doc_paths]\nsrc_doc_vecs = []\nfor p in tqdm(src_doc_paths, desc=\"Embedding src docs\", ncols=100):\n    text, _ = load_and_split(p)\n    vec = model.encode([text], convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)\n    src_doc_vecs.append(vec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:43:27.687504Z","iopub.execute_input":"2025-05-27T14:43:27.688079Z","iopub.status.idle":"2025-05-27T14:45:30.181891Z","shell.execute_reply.started":"2025-05-27T14:43:27.688059Z","shell.execute_reply":"2025-05-27T14:45:30.180773Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Embedding src docs:  15%|█████▊                                 | 1183/7949 [02:02<11:39,  9.67it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2245923019.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_doc_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Embedding src docs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msrc_doc_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Batches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0msentences_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \"\"\"\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sentence_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts, padding)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         output.update(\n\u001b[0;32m--> 500\u001b[0;31m             self.tokenizer(\n\u001b[0m\u001b[1;32m    501\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mto_tokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2885\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2886\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2887\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2973\u001b[0m                 )\n\u001b[1;32m   2974\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2976\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3175\u001b[0m         )\n\u001b[1;32m   3176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3177\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3178\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3179\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"markdown","source":"✅ 推荐保存格式：.npz 压缩向量字典\n✅ 保存到 .npz 文件（当前 Notebook 最后执行一次）：","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# 建立 ID → 向量的映射（dict）\nsrc_doc_dict = {\n    doc_name.replace(\".txt\", \"\"): vec\n    for doc_name, vec in zip(src_doc_names, src_doc_vecs)\n}\n\n# 保存为压缩文件\nnp.savez_compressed(\"validation_src_doc_vecs_e5.npz\", **src_doc_dict)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:45:41.743468Z","iopub.execute_input":"2025-05-27T14:45:41.743763Z","iopub.status.idle":"2025-05-27T14:45:41.940201Z","shell.execute_reply.started":"2025-05-27T14:45:41.743742Z","shell.execute_reply":"2025-05-27T14:45:41.939478Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"✅ 下次使用时只需几行代码：","metadata":{}},{"cell_type":"code","source":"# 指定你上传的数据集路径\nVEC_PATH = \"/kaggle/input/plagiarism-detection-data/validation_src_doc_vecs_e5.npz\"\nSUSP_VEC_DIR =\"/kaggle/input/plagiarism-detection-data/susp_cache/kaggle/working/susp_emb_cache_e5\"\n# 加载所有嵌入\nsrc_vec_npz = np.load(VEC_PATH)\n\n# 获取单篇向量（例如：document123.txt）\n# vec = src_vec_npz[\"document123\"]\nsrc_doc_names = list(src_vec_npz.keys())\nsrc_doc_vecs = [src_vec_npz[k] for k in src_doc_names]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:45:45.725890Z","iopub.execute_input":"2025-05-27T14:45:45.726466Z","iopub.status.idle":"2025-05-27T14:45:48.672389Z","shell.execute_reply.started":"2025-05-27T14:45:45.726445Z","shell.execute_reply":"2025-05-27T14:45:48.671802Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"go on","metadata":{}},{"cell_type":"code","source":"# def get_top_k_sources(susp_path, k=5):\n#     # susp_vec = get_doc_embedding(susp_path)  # shape: (384,)\n#     susp_vec = np.load(os.path.join(SUSP_VEC_DIR, susp_name.replace(\".txt\", \".npy\")))\n#     sims = np.dot(src_doc_vecs, susp_vec)    # shape: (num_src,)\n#     topk_idx = sims.argsort()[-k:][::-1].tolist()\n#     # top_src_names = [src_doc_names[int(i) if not isinstance(i, list) else int(i[0])] for i in topk_idx]\n#     # return top_src_names\n#     # sims = np.dot(src_doc_vecs, susp_vec)\n#     idxs = np.argsort(sims)[::-1][:top_k]\n#     return [src_doc_names[i] for i in idxs]\n\n\n# def get_top_k_sources(susp_name, k=10):\n#     susp_vec_path = os.path.join(SUSP_VEC_DIR, susp_name.replace(\".txt\", \".npy\"))\n#     if not os.path.exists(susp_vec_path):\n#         print(f\"❌ Susp 向量缺失: {susp_name}\")\n#         return []\n\n#     susp_vec = np.load(susp_vec_path)  # shape: (768,)\n    \n#     sims = np.dot(src_doc_vecs, susp_vec)\n#     idxs = np.argsort(sims)[::-1][:top_k]\n\n#     return [src_doc_names[i] for i in idxs]\n\ndef get_top_k_sources(susp_name, k):\n    susp_basename = os.path.basename(susp_name)\n    susp_vec_path = os.path.join(SUSP_VEC_DIR, susp_basename.replace(\".txt\", \".npy\"))\n\n    if not os.path.exists(susp_vec_path):\n        print(f\"❌ Susp 向量缺失: {susp_basename}\")\n        return []\n\n    susp_vec = np.load(susp_vec_path)\n    sims = np.dot(src_doc_vecs, susp_vec)\n    idxs = np.argsort(sims)[::-1][:k]\n    return [src_doc_names[i] for i in idxs]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:58:39.242046Z","iopub.execute_input":"2025-05-27T14:58:39.242611Z","iopub.status.idle":"2025-05-27T14:58:39.247723Z","shell.execute_reply.started":"2025-05-27T14:58:39.242593Z","shell.execute_reply":"2025-05-27T14:58:39.247057Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"!pip install tqdm-joblib --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:45:30.186559Z","iopub.status.idle":"2025-05-27T14:45:30.186877Z","shell.execute_reply.started":"2025-05-27T14:45:30.186728Z","shell.execute_reply":"2025-05-27T14:45:30.186742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np, os\nfrom tqdm import tqdm\n\nmodel = SentenceTransformer(\"intfloat/e5-base-v2\")\nmodel.to(\"cuda\")\n\n\nout_dir = \"susp_emb_cache_e5\"\nos.makedirs(out_dir, exist_ok=True)\n\nfor fname in tqdm(os.listdir(SUSP_DIR)):\n    if not fname.endswith(\".txt\"): continue\n    path = os.path.join(SUSP_DIR, fname)\n    with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n        text = f.read()\n\n    emb = model.encode([text], convert_to_numpy=True,\n                       normalize_embeddings=True)[0]\n    np.save(os.path.join(out_dir, fname.replace(\".txt\", \".npy\")), emb)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:46:03.938739Z","iopub.execute_input":"2025-05-27T14:46:03.939260Z","iopub.status.idle":"2025-05-27T14:46:08.863754Z","shell.execute_reply.started":"2025-05-27T14:46:03.939238Z","shell.execute_reply":"2025-05-27T14:46:08.862686Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"  0%|          | 0/7950 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfdc7c9fb5ae4c4c84ece8f42b2c2f8a"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 1/7950 [00:00<16:55,  7.83it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1ac622767994e2f8742980ce7d496f9"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 2/7950 [00:00<16:43,  7.92it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d25b474682e74a3da2b60f801e6d98e5"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 3/7950 [00:00<15:21,  8.63it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29930a8487814c939b524436731b6a90"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 4/7950 [00:00<14:39,  9.04it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d01fbf34ae1c474589065c086c95161a"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 5/7950 [00:00<16:11,  8.18it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f84b1b55975344698124b2a4fc400fbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cd6b6f7cc54435ba0858cda807bb8d5"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 7/7950 [00:00<13:38,  9.71it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bc803da107e41a3b4042c765e497da7"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 8/7950 [00:00<14:27,  9.15it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e27034a80a12450491aef0e2b65bda7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03e3576a05204b49a5ef493704e826f7"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 10/7950 [00:01<12:52, 10.28it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3737c2812db414c89a118d75fab42a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe6df5520e249379ec469909ead1c00"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 12/7950 [00:01<12:45, 10.37it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5371ad532cd44350b1552e76a2991e68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3919432f006a461ca7c5c3a02ffcd3ba"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 14/7950 [00:01<14:24,  9.18it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ba049015c59403ebedda6a574e48aa7"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 15/7950 [00:01<16:16,  8.13it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f30db73b6604d2a98b28b3d2082eaff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abc9f5a11a344f5d9cd2c54c4c567d75"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 17/7950 [00:01<14:26,  9.15it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40e3fe02d9b5417db728721316f5399f"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 18/7950 [00:02<18:55,  6.99it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fac86b3bee24482db447fd0d7a77c24e"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 19/7950 [00:02<18:01,  7.33it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56b346b69cf14399a57809a6b5834189"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 20/7950 [00:02<16:50,  7.85it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8580169b0414d83b3cd20fc1b09b87a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe309b47e25d4970909e28d0d8b0dedf"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 22/7950 [00:02<14:50,  8.91it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de3449b9b6a244bdb6d0cc90fe333f03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a9594264b6c40c1ac7b8cbcdbbb5b4f"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 24/7950 [00:02<14:18,  9.23it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17fad284c9314c83acd1fafba8c088ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94733a24832443948a3fe9c640d3b74d"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 26/7950 [00:02<13:14,  9.98it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c7ffede10fb4e1d8eada972aea8fd59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bb4dfb8804640af8b3653981d100b24"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 28/7950 [00:03<13:23,  9.86it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c8208cd24bb4a0680b40d7dba63d7e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52f8c0c543a94ffe8dbe0d94746007e1"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 30/7950 [00:03<14:03,  9.39it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6b7e063509143c6a0cbceb2d01ab539"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 31/7950 [00:03<15:16,  8.64it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13190c0b94ff403bac0c3d21244fcac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66f439e3e90a4c7bb78e8777b6eeb0e8"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 33/7950 [00:03<13:57,  9.45it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"410a0a42ff694325a55117b6490a0ac4"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 33/7950 [00:03<14:55,  8.84it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1896716778.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     emb = model.encode([text], convert_to_numpy=True,\n\u001b[0m\u001b[1;32m     20\u001b[0m                        normalize_embeddings=True)[0]\n\u001b[1;32m     21\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Batches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0msentences_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \"\"\"\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sentence_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts, padding)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         output.update(\n\u001b[0;32m--> 500\u001b[0;31m             self.tokenizer(\n\u001b[0m\u001b[1;32m    501\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mto_tokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2885\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2886\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2887\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2973\u001b[0m                 )\n\u001b[1;32m   2974\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2976\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3175\u001b[0m         )\n\u001b[1;32m   3176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3177\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3178\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3179\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"import os\n\n# 替换为实际路径\nvec_file = \"/kaggle/input/plagiarism-detection-data/susp_cache/kaggle/working/susp_emb_cache_e5/suspicious-document010240.npy\"\nprint(os.path.exists(vec_file))  # 应该是 True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:45:57.375009Z","iopub.execute_input":"2025-05-27T14:45:57.375429Z","iopub.status.idle":"2025-05-27T14:45:57.399552Z","shell.execute_reply.started":"2025-05-27T14:45:57.375397Z","shell.execute_reply":"2025-05-27T14:45:57.398608Z"}},"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from joblib import Parallel, delayed\nfrom tqdm import tqdm\nfrom tqdm_joblib import tqdm_joblib\nimport multiprocessing, time, os, warnings\n\nimport os\nimport warnings\n\n\n# 屏蔽所有 Python 警告\nwarnings.filterwarnings(\"ignore\")\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 消除 tokenizer 并行警告\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"        # 关闭 TensorFlow 初始化错误输出\n\n# 环境配置\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nwarnings.filterwarnings(\"ignore\")\nNUM_CORES = min(2, multiprocessing.cpu_count())\n\n# 加载文档对\npairs = [tuple(line.strip().split()) for line in open(PAIRS_FILE, encoding='utf-8')]\n\nwith open(PAIRS_FILE, encoding='utf-8') as f:\n    for line in f:\n        susp_name, src_name = line.strip().split()\n        # 自动补全 .txt（如果缺失）\n        if not susp_name.endswith(\".txt\"):\n            susp_name += \".txt\"\n        if not src_name.endswith(\".txt\"):\n            src_name += \".txt\"\n        pairs.append((susp_name, src_name))\n\nsusp_files = sorted(set([s for s, _ in pairs]))\n\n# 包装函数\ndef safe_process_pair(susp_name, src_name):\n    try:\n        process_pair(susp_name, src_name)\n    except Exception as e:\n        print(f\"❌ 错误: {susp_name} - {src_name}，{e}\")\n\nNUM_CORES = 2  # Kaggle 最多 2 核\nstart = time.time()\n\nwith tqdm_joblib(tqdm(total=len(pairs), desc=\"📄 并行处理文档对\", unit=\"pair\")):\n    # 并行：\n    Parallel(n_jobs=2)(\n        delayed(process_susp_filtered)(susp_name)\n        for susp_name in susp_files\n    )\n\n\nprint(f\"\\n✅ 全部完成！总耗时 {time.time() - start:.2f} 秒\")\nprint(f\"✅ 总处理数：{len(pairs)}，成功命中：{stats['matched']}，无匹配：{stats['empty']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:58:43.473212Z","iopub.execute_input":"2025-05-27T14:58:43.473487Z","iopub.status.idle":"2025-05-27T14:58:57.164949Z","shell.execute_reply.started":"2025-05-27T14:58:43.473467Z","shell.execute_reply":"2025-05-27T14:58:57.163379Z"}},"outputs":[{"name":"stderr","text":"\n\n\n\n📄 并行处理文档对:   0%|          | 0/15952 [00:00<?, ?pair/s]\u001b[A\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15952 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c45b6d87304e3ab417f990af0a3388"}},"metadata":{}},{"name":"stderr","text":"📄 并行处理文档对:   0%|          | 0/15952 [12:29<?, ?pair/s]\n📄 并行处理文档对:   0%|          | 0/15952 [07:04<?, ?pair/s]\n📄 并行处理文档对:   0%|          | 0/15952 [05:00<?, ?pair/s]\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748357932.259726     252 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748357932.267659     252 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748357934.176626     253 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748357934.183727     253 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)","\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py\", line 490, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 606, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 606, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_35/1476023372.py\", line 82, in process_susp_filtered\n  File \"/tmp/ipykernel_35/1212615736.py\", line 37, in get_top_k_sources\n  File \"/tmp/ipykernel_35/1212615736.py\", line 37, in <listcomp>\nTypeError: only integer scalar arrays can be converted to a scalar index\n\"\"\"","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2163715959.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtqdm_joblib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"📄 并行处理文档对\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pair\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# 并行：\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     Parallel(n_jobs=2)(\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_susp_filtered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msusp_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msusp_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msusp_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2069\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2071\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1781\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1856\u001b[0m         \u001b[0;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m             \u001b[0merror_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# be returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"],"ename":"TypeError","evalue":"only integer scalar arrays can be converted to a scalar index","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"import glob\nprint(\"实际输出文件数量：\", len(glob.glob(os.path.join(OUTPUT_DIR, \"*.xml\"))))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:45:30.193043Z","iopub.status.idle":"2025-05-27T14:45:30.193342Z","shell.execute_reply.started":"2025-05-27T14:45:30.193181Z","shell.execute_reply":"2025-05-27T14:45:30.193197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"示例路径：\", os.path.join(SUSP_DIR, \"suspicious-document020468.txt\"))\nprint(\"存在吗？\", os.path.exists(os.path.join(SUSP_DIR, \"suspicious-document020468.txt\")))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:45:30.195012Z","iopub.status.idle":"2025-05-27T14:45:30.195312Z","shell.execute_reply.started":"2025-05-27T14:45:30.195149Z","shell.execute_reply":"2025-05-27T14:45:30.195161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"模型训练与评估\n","metadata":{}},{"cell_type":"code","source":"# 示例：压缩 /kaggle/working/output_folder 为 output.zip\n# !zip -r output.zip /kaggle/working/validation_pred_xml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:45:30.196576Z","iopub.status.idle":"2025-05-27T14:45:30.196811Z","shell.execute_reply.started":"2025-05-27T14:45:30.196702Z","shell.execute_reply":"2025-05-27T14:45:30.196712Z"}},"outputs":[],"execution_count":null}]}